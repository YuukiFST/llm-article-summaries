Reference: https://www.arxiv.org/pdf/2601.07372

# Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models

# Memória Condicional em LLMs: O Paradigma Engram e a Nova Dimensão de Esparsidade

### Executive Summary

O artigo *Conditional Memory via Scalable Lookup*, desenvolvido por pesquisadores da Peking University e da DeepSeek-AI, propõe uma mudança fundamental na arquitetura de Large Language Models (LLMs). Tradicionalmente, modelos como Transformers baseiam-se em esparsidade de *condicional computation* (ex: Mixture-of-Experts - MoE) para escalar capacidade. Este trabalho introduz um eixo complementar: a **memória condicional**.

Por meio de um módulo denominado **Engram**, os autores modernizam a técnica clássica de N-gramos, permitindo recuperação de conhecimento estático em tempo O(1). O estudo formula o problema de "Sparsity Allocation", revelando uma **lei de escala em formato de U** que define o equilíbrio ideal entre computação neural (MoE) e memória estática (Engram). Ao escalar o Engram para 27B parâmetros, o modelo supera consistentemente baselines estritos de iso-parâmetro e iso-FLOPs baseados apenas em MoE, com ganhos notáveis não apenas em recuperação de conhecimento, mas surpreendentemente em raciocínio complexo e tarefas de longo contexto.

### Análise Técnica

#### 1. Arquitetura

O núcleo da proposta é o módulo **Engram**, projetado para separar estruturadamente o armazenamento de padrões estáticos da computação dinâmica.

*   **Recuperação Esparsa via N-gramos Hashing:**
    *   Em vez de forçar o Transformer a reconstruir entidades e padrões locais através de múltiplas camadas de atenção (o que consome profundidade computacional), o Engram utiliza o contexto local (N-gramos de tokens anteriores) como uma chave de busca.
    *   Para lidar com a explosão combinatória de possíveis N-gramos, utiliza-se **Hashing Multipartidário**. Cada cabeça de hash mapeia o contexto comprimido para um índice em uma tabela de embeddings massiva.
    *   Implementa-se a **Compressão de Tokenizer** (normalização textual) para aumentar a densidade semântica, reduzindo o vocabulário efetivo em ~23%.

*   **Fusão Sensível ao Contexto (Context-aware Gating):**
    *   Como as memórias recuperadas são estáticas, elas carecem de adaptação contextual. O módulo utiliza um mecanismo de *gating* semelhante à Atenção, onde o estado oculto atual ($h_t$) atua como *Query* e a memória recuperada como *Key/Value*.
    *   Isso permite que o modelo suprima ruídos ou memórias irrelevantes para o contexto atual, resultando em um vetor de saída dinâmico.
    *   Uma convolução causal leve é aplicada posteriormente para expandir o campo receptivo.

#### 2. Metodologia e Escalabilidade

O estudo aborda a escalabilidade sob duas óticas principais:

*   **O Problema da Alocação de Esparsidade (Sparsity Allocation):**
    *   Dado um orçamento fixo de parâmetros totais ($P_{tot}$) e FLOPs ($P_{act}$), como dividir o orçamento "grátis" de parâmetros inativos ($P_{sparse}$) entre Especialistas MoE e Tabelas de Memória Engram?
    *   Descobriu-se uma relação em **formato U**: Alocar tudo para MoE ($\rho = 1$) é subótimo (falta de memória estática), mas alocar tudo para Engram ($\rho = 0$) também é ruim (falta de computação dinâmica).
    *   O ponto ótimo ocorre em torno de $\rho \approx 75\%$ - $80\%$, ou seja, realocando 20%-25% da capacidade dos especialistas para memória de N-gramos.

*   **Lei de Escala "Infinite Memory":**
    *   Testou-se o comportamento do modelo ao aumentar agressivamente o tamanho das tabelas de embedding. Os resultados mostram um declínio consistente e linear (em escala log-log) da *validation loss*, indicando que a memória condicional é um eixo de escala viável e previsível que não aumenta custo de inferência.

#### 3. Resultados

A comparação foca no modelo **Engram-27B** (26.7B params) contra um baseline **MoE-27B** estrito, ambos treinados por 262B tokens.

*   **Geral e Conhecimento:** Melhorias esperadas em tarefas factuais.
    *   MMLU: +3.4 pontos (60.4 vs 57.4).
    *   CMMLU: +4.0 pontos.

*   **Raciocínio e Código:** Ganhos inesperados e significativos, sugerindo que o Engram libera capacidade do backbone para processar lógica complexa.
    *   BBH (Big-Bench Hard): +5.0 pontos (55.9 vs 50.9).
    *   ARC-Challenge: +3.7 pontos.
    *   HumanEval: +3.0 pontos.
    *   MATH: +2.4 pontos.

*   **Contexto Longo:** O Engram alivia a atenção de ter que lidar com dependências locais imediatas (já resolvidas pelo lookup), focando no contexto global.
    *   No benchmark **RULER**, o Engram-27B superou o MoE-27B substancialmente. Ex: Multi-Query NIAH (Needle-in-a-Haystack) saltou de 84.2 para 97.0 de acurácia.

#### 4. Análise Mecanicista e Eficiência de Sistema

*   **Profundidade Efetiva:** Análises com *LogitLens* e *CKA* revelaram que as primeiras camadas do modelo Engram convergem para predições finais muito mais rápido que o baseline MoE. Funcionalmente, o Engram "pula" as camadas iniciais de reconstrução de entidades, agindo como se tivesse aumentado a profundidade efetiva da rede.
*   **Eficiência de Infraestrutura:** Devido ao endereçamento determinístico (baseado nos IDs dos tokens, não nos estados ocultos dinâmicos como no MoE), o sistema pode fazer *prefetching* de memória. Isso permite que tabelas massivas (ex: 100B parâmetros) sejam movidas para a memória da CPU (host) com overhead negligível (<3%), contornando limites de VRAM.

### Key Takeaways

1.  **A Volta dos N-gramos:** Modelos clássicos de linguagem (N-gramos) não estão obsoletos; quando integrados como primitivos de busca O(1) dentro de Transformers modernos, eles oferecem benefícios massivos de eficiência.
2.  **Compute vs. Memória:** A esparsidade não deve se limitar a ativação de computação (MoE). A memória estática (lookup) é um eixo de escala distinto e complementar.
3.  **Raciocínio Auxiliado por Memória:** Acesso rápido a conhecimento estático não ajuda apenas na recuperação de fatos (trivia), mas liberta recursos computacionais para raciocínio abstrato (BBH, MATH), demonstrando uma sinergia arquitetural profunda.
4.  **Determinismo é Eficiência:** A natureza determinística dos lookups do Engram permite otimizações de sistema (offloading/prefetching) que são difíceis ou impossíveis em roteamentos dinâmicos como MoE, permitindo escalabilidade de parâmetros sem explosão de custo de infraestrutura.

### Conclusão

O módulo Engram representa um passo evolucionário na arquitetura de LLMs, validando a "memória condicional" como um bloco de construção fundamental. Ao aliviar o backbone da reconstrução de padrões estáticos e dependências locais, o modelo alcança um "ganho livre" em profundidade efetiva e capacidade de contexto global. Para futuros modelos de fronteira, ignorar este eixo de esparsidade pode significar desperdiçar capacidade computacional em operações triviais que poderiam ser resolvidas por um simples, porém massivo, *lookup table*.

</FINAL_ANSWER>
